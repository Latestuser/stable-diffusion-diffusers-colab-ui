{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "c7VG6GVTS0Al"
      ],
      "authorship_tag": "ABX9TyM3BAyDRRJB/JhSJhIbKlG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oneir0mancer/stable-diffusion-diffusers-colab-ui/blob/main/sd_diffusers_colab_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "c7VG6GVTS0Al"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-z4MGy9rz9I"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade diffusers accelerate transformers xformers safetensors\n",
        "!mkdir -p outputs/{txt2img,img2img}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/oneir0mancer/stable-diffusion-diffusers-colab-ui.git\n",
        "!git clone https://huggingface.co/embed/negative /content/embeddings/negative\n",
        "!git clone https://huggingface.co/embed/lora /content/Lora/positive"
      ],
      "metadata": {
        "id": "H-XcdgK8yboS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "output_index = 0\n",
        "generator = torch.Generator()\n",
        "\n",
        "# Default settings\n",
        "save_images = True\n",
        "display_images = True"
      ],
      "metadata": {
        "id": "DG4psBfYsUp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating model pipeline\n",
        "You can use most from [huggingface](https://huggingface.co). Just make sure they are in diffusers format, check their **Files** and see if there is a `model_index.json`.\n",
        "\n",
        "I made a simple index wih popular models, so you can just render UI and choose a model from dropdown. Some models require trigger word in the prompt.\n",
        "\n",
        "For weights in Automatic111 format (**.ckpt** or **.safetensors** but without model_index.json), you'll need to download them and convert using scripts (WIP)."
      ],
      "metadata": {
        "id": "dVlvLpShS9eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Render model choice UI\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "\n",
        "with open(\"/content/stable-diffusion-diffusers-colab-ui/model_index.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[x for x in data],\n",
        "    description=\"Model:\",\n",
        ")\n",
        "model_link = widgets.HTML()\n",
        "\n",
        "def set_label_from_dict(key):\n",
        "    model_link.value = f\"Model info: <a href=https://huggingface.co/{data[key]['id']}>{key}</a>\"\n",
        "    try:\n",
        "        model_link.value += f\"<br>Trigger prompt: <code>{data[key]['trigger']}</code>\"\n",
        "    except: pass\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "    set_label_from_dict(change.new)\n",
        "\n",
        "set_label_from_dict(model_dropdown.value)\n",
        "model_dropdown.observe(dropdown_eventhandler, names='value')\n",
        "\n",
        "display(model_dropdown, model_link)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k_TR5dUwznN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load chosen model\n",
        "#@markdown Alternatively you can just paste huggingface model_id or path to model folder here:\n",
        "\n",
        "path_to_model = \"\"  #@param {type: \"string\"}\n",
        "\n",
        "if path_to_model != \"\": model_id = path_to_model\n",
        "else: model_id = data[model_dropdown.value]['id']\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.safety_checker = None"
      ],
      "metadata": {
        "id": "IBdm3HvI02Yo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VAE\n",
        "\n",
        "Models loaded from huggingface should have proper VAE. But you also can load VAE from any other repository from [huggingface](https://huggingface.co), just paste their **model_id** and subfolder (usually just \"vae\"). \n",
        "\n",
        "Or you can load it in Automatic111 format and convert using scripts."
      ],
      "metadata": {
        "id": "hEwlf_SQXCyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Load VAE\n",
        "\n",
        "from diffusers import AutoencoderKL\n",
        "\n",
        "vae_id_or_path = \"runwayml/stable-diffusion-v1-5\"  #@param {type: \"string\"}\n",
        "vae_subfolder = \"vae\"    #@param {type: \"string\"}\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"vae_id_or_path\", subfolder=vae_subfolder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.vae = vae"
      ],
      "metadata": {
        "id": "8jKsuQ9E5eCK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Textual inversions and LoRAs\n",
        "This is work in progress. \n",
        "\n",
        "You can load any textual inversions using \n",
        "```\n",
        "pipe.load_textual_inversion(\"path/to/dir\", weight_name=\"filename.pt\")\n",
        "```\n",
        "To use them, you need to add theit token to a prompt like this `<filename>`.\n",
        "\n",
        "Note, that it seems that if you change VAE after loading textual inversions, using them will degrade generated images."
      ],
      "metadata": {
        "id": "VDQSRzWg5OKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Load textual inversions\n",
        "#@markdown Load every embedding with **.pt** extension from **embeddings** folder.\n",
        "\n",
        "import os\n",
        "\n",
        "for path, subdirs, files in os.walk(\"/content/embeddings/\"):\n",
        "    for name in files:\n",
        "        if os.path.splitext(name)[1] != \".pt\": continue\n",
        "        pipe.load_textual_inversion(path, weight_name=name)\n",
        "        print(path, name)"
      ],
      "metadata": {
        "id": "VVU2Rjpb085a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating images"
      ],
      "metadata": {
        "id": "hLmzZmRIZMpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "#@markdown Run this cell to update\n",
        "save_images = True #@param {type:\"boolean\"}\n",
        "display_images = True    #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oFruEVcl4ODt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell to generate images\n",
        "prompt = \"best quality, 1girl, illustration\"  #@param {type: \"string\"}\n",
        "negative_prompt = \"<EasyNegative>, by <bad-artist-anime>, worst quality, awful quality\"   #@param {type: \"string\"}\n",
        "\n",
        "width = 512     #@param {type:\"integer\"}\n",
        "height = 768    #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "steps = 20  #@param {type: \"slider\", min: 1, max: 100}\n",
        "CFG = 7     #@param {type: \"slider\", min: 1, max: 20}\n",
        "seed = -1   #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "batch_size = 1   #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "if seed >= 0: generator = torch.manual_seed(seed)\n",
        "\n",
        "results = pipe([prompt]*batch_size, negative_prompt=[negative_prompt]*batch_size, \n",
        "               num_inference_steps=steps, guidance_scale=CFG, \n",
        "               generator=generator, height=height, width=width)\n",
        "\n",
        "# Save and display\n",
        "for image in results.images:\n",
        "    if save_images:\n",
        "        image.save(f\"outputs/txt2img/{output_index:05}.png\")\n",
        "        print(f\"outputs/txt2img/{output_index:05}.png\")\n",
        "        output_index += 1\n",
        "    if display_images: \n",
        "        display(image)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fqUSaM3WteZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}